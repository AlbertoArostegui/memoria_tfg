\section{Implementación}
A lo largo de esta sección se va a detallar la implementación de las distintas propuestas tecnológicas que se han descrito en el apartado anterior.

Para la implementación y el lanzamiento de muchas pruebas, se ha decidido ejecutar un LLM de manera local, ya que, de esta, manera, reducimos el coste de las llamadas a OpenAI a tan solo aquel que acarree el cálculo de los embeddings.

El modelo a utilizar será Llama3:8b, ejecutado en local mediante Ollama~\footnote{https://ollama.com} y al cual se harán llamadas a traves de programas Python, utilizando la librería Langchain~\footnote{https://www.langchain.com/}.

\input{implementacion/impl_embe.tex}
\input{implementacion/impl_onto.tex}