\subsection{Modelos grandes de lenguaje}
Dentro del campo de la inteligencia artificial y el \aclink{PLN}, los modelos grandes de lenguaje, conocidos en inglés como \aclink{LLM} han sido una de las tecnologías más revolucionarias de los últimos años.

Los \ac{LLM}s se basan en arquitecturas de redes neuronales profundas, como los \textit{transformers} \cite{attention2017}, que permiten procesar secuencias de texto de manera más eficiente. Gracias a su mecanismo de atención, el cual permite al modelo enfocarse en las partes más relevantes de la secuencia de texto, los transformers han sido la base de muchos de los modelos de lenguaje más grandes y potentes de la actualidad.

A diferencia de modelos lingüísticos anteriores, los \ac{LLM}s son capaces de aprender de manera no supervisada, lo que les permite obtener información de grandes cantidades de texto sin necesidad de etiquetas. Esto ha permitido el desarrollo de modelos masivos, como GPT-3 \cite{gpt32020}, Gemini, LLama o Claude, que han demostrado ser capaces de realizar tareas de generación de texto, traducción, resumen, entre otras, con resultados sorprendentes.

Estos modelos de lenguaje tan grandes han sido entrenados con inmensas cantidades de texto, lo que les ha permitido aprender de una gran cantidad de información y generar texto de manera coherente para una gran cantidad de contextos. Sin embargo, estos modelos son muy costosos de entrenar y de mantener, ya que requieren de una gran cantidad de recursos computacionales y de memoria para funcionar correctamente. Además, los datos con los que han sido entrenados no están disponibles en su gran mayoría para el público general, lo que, junto con su coste, ha provocado la popularización de estos como servicios en la nube, en contraposición a una herramienta que se pueda ejecutar en local.