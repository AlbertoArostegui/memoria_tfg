\subsection{Word embeddings}
Los word embeddings, o simplemente embeddings, son una representación de palabras en un espacio multidimensional y son un técnica fundamental dentro del campo del \aclink{PLN}. Consiste en la transformación de palabras en vectores de números reales dentro del espacio, de manera que se puedan realizar operaciones matemáticas con ellas.

Para transformar las palabras en vectores se busca capturar la semántica de las palabras, de manera que palabras similares tengan vectores cercanos. Los métodos tradicionales de embeddings, como Word2vec \cite{mikolov2013efficient} o GloVe \cite{pennington2014glove}, utilizan modelos de aprendizaje no supervisado para aprender la representación de las palabras a partir de grandes cantidades de texto.

Los embeddings tienen muchas aplicaciones, como la clasificación de texto, la traducción automática o la recuperación de información.